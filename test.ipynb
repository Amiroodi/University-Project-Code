{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3992, 0.3992])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "loss_fn(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0479, 0.6709, 0.2233, 0.0377, 0.0203]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft = torch.softmax(y_pred, dim=1)\n",
    "soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0392, 0.3992, 1.4992, 3.2792, 3.8992]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = torch.log(soft)\n",
    "-log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def focal_loss_multiclass(inputs, targets, alpha=1, gamma=2):\n",
    "    \"\"\"\n",
    "    Multi-class focal loss implementation\n",
    "    - inputs: raw logits from the model\n",
    "    - targets: true class labels (as integer indices, not one-hot encoded)\n",
    "    \"\"\"\n",
    "    # Convert logits to log probabilities\n",
    "    log_prob = F.log_softmax(inputs, dim=-1)\n",
    "    prob = torch.exp(log_prob)  # Calculate probabilities from log probabilities\n",
    "\n",
    "    # Gather the probabilities corresponding to the correct classes\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[-1])\n",
    "    pt = torch.sum(prob * targets_one_hot, dim=-1)\n",
    "\n",
    "    # Apply focal adjustment\n",
    "    focal_loss = -alpha * (1 - pt) ** gamma * torch.sum(log_prob * targets_one_hot, dim=-1)\n",
    "    \n",
    "    return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0432)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focal_loss_multiclass(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "alpha = torch.tensor([0.1, 1, 0.5, 0.7, 0.7])\n",
    "\n",
    "y_pred = torch.tensor([[0.56, 3.2, 2.1, 0.32, -0.3], [0.56, 3.2, 2.1, 0.32, -0.3]])\n",
    "y_class = torch.tensor([1, 1])\n",
    "y_ord = torch.tensor([[1, 1, 1, 1, 0], [1, 1, 1, 1, 1]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1]])\n",
      "tensor([3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(y_ord.to(dtype=torch.int64))\n",
    "print(torch.sum(y_ord.to(dtype=torch.int64), dim=1) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha, gamma, headType=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.headType = headType\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        if self.headType == 'classification':\n",
    "            ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "            # apply class weights\n",
    "            # alpha is for the weight class, weights is the correct weight for each class and looks like this: tensor([a, b, c, d, a, ...])\n",
    "            weights = self.alpha.gather(0, targets)\n",
    "\n",
    "        if self.headType == 'ordinal':\n",
    "            ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "            # apply class weights\n",
    "            # alpha is for the weight class, weights is the correct weight for each class and looks like this: tensor([a, b, c, d, a, ...])\n",
    "            weights = self.alpha\n",
    "\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        print('pt is: ', pt)\n",
    "        print('ce_loss', ce_loss)\n",
    "        loss = (weights * ((1 - pt) ** self.gamma) * ce_loss)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt is:  tensor([0.6709, 0.6709])\n",
      "ce_loss tensor([0.3992, 0.3992])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1314)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = FocalLoss(headType='classification', alpha=alpha, gamma=1)\n",
    "loss_fn(y_pred, y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt is:  tensor([[0.6365, 0.9608, 0.8909, 0.5793, 0.5744],\n",
      "        [0.6365, 0.9608, 0.8909, 0.5793, 0.4256]])\n",
      "ce_loss tensor([[0.4518, 0.0400, 0.1155, 0.5459, 0.5544],\n",
      "        [0.4518, 0.0400, 0.1155, 0.5459, 0.8544]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2036)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = FocalLoss(headType='ordinal', alpha=alpha, gamma=0)\n",
    "loss_fn(y_pred, y_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4518, 0.0400, 0.1155, 0.5459, 0.5544],\n",
      "        [0.4518, 0.0400, 0.1155, 0.5459, 0.8544],\n",
      "        [0.1269, 0.1269, 0.0486, 0.3133, 0.0067]])\n",
      "tensor(0.2892)\n"
     ]
    }
   ],
   "source": [
    "y_logits = torch.tensor([[0.56, 3.2, 2.1, 0.32, -0.3], [0.56, 3.2, 2.1, 0.32, -0.3], [2, 2, 3, 1, -5]])\n",
    "y_class = torch.tensor([1, 1])\n",
    "y_ord = torch.tensor([[1, 1, 1, 1, 0], [1, 1, 1, 1, 1], [1, 1, 1, 1, 0]]).float()\n",
    "\n",
    "print(F.binary_cross_entropy_with_logits(y_logits, y_ord, reduction='none'))\n",
    "print(F.binary_cross_entropy_with_logits(y_logits, y_ord, reduction='mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6364)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import tensor\n",
    "from torchmetrics.classification import MulticlassCohenKappa\n",
    "target = tensor([2, 1, 0, 1])\n",
    "preds = tensor([2, 1, 0, 0])\n",
    "metric = MulticlassCohenKappa(num_classes=3)\n",
    "metric(preds, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
